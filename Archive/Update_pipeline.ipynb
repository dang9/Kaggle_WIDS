{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# additional experiement packages\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset \n",
    "training = pd.read_csv(\"./input/training_v2.csv\")\n",
    "solution_template = pd.read_csv(\"./input/solution_template.csv\")\n",
    "samplesubmission = pd.read_csv(\"./input/samplesubmission.csv\")\n",
    "unlabeled = pd.read_csv(\"./input/unlabeled.csv\")\n",
    "dictionary = pd.read_csv(\"./input/WiDS Datathon 2020 Dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =  training.drop(columns=['hospital_death']), training[['hospital_death']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73370, 185) (18343, 185) (73370, 1) (18343, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create transformers for pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "## data needs to be at preprocessing stage\n",
    "# need to undo steps below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputer. Replace NaN with 'missing_value' for categorical fields.\n",
    "categorical_transformer  = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant')),\n",
    "                                           ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Standardise numerical fields\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of features with specific data types for processing\n",
    "\n",
    "numeric_features     = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble categorical and numerical pipelines\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('num',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('imputer',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='mean',\n",
       "                                                                verbose=0)),\n",
       "                                                 ('scaler',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True))],\n",
       "                                          verbose=False),\n",
       "                                 Index(['...\n",
       "                                                 ('encoder',\n",
       "                                                  OneHotEncoder(categorical_features=None,\n",
       "                                                                categories=None,\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='ignore',\n",
       "                                                                n_values=None,\n",
       "                                                                sparse=True))],\n",
       "                                          verbose=False),\n",
       "                                 Index(['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',\n",
       "       'icu_stay_type', 'icu_type', 'apache_3j_bodysystem',\n",
       "       'apache_2_bodysystem'],\n",
       "      dtype='object'))],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/10592605/save-classifier-to-disk-in-scikit-learn\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# now you can save it to a file\n",
    "# joblib.dump(preprocessor, 'preprocessor.pkl') \n",
    "\n",
    "# and later you can load it\n",
    "# preprocessor_load = joblib.load('preprocessor.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_columns(df):\n",
    "    list_to_drop = df.columns[(df.isnull().sum()/df.shape[0]) > 0.1]\n",
    "    df.drop(columns=list_to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_rows(df):\n",
    "    rows_to_drop = df[(df.isnull().sum(axis=1)/df.shape[1]) > 0.2].index\n",
    "    df.drop(rows_to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of BaseEstimator()>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Base class for all estimators in scikit-learn\n",
       "\n",
       "Notes\n",
       "-----\n",
       "All estimators should specify all the parameters that can be set\n",
       "at the class level in their ``__init__`` as explicit keyword\n",
       "arguments (no ``*args`` or ``**kwargs``).\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\anthony\\anaconda3\\lib\\site-packages\\sklearn\\base.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     SimpleImputer, MissingIndicator, _BaseComposition, FunctionTransformer, LabelEncoder, LabelBinarizer, MultiLabelBinarizer, _BaseEncoder, MinMaxScaler, StandardScaler, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://github.com/jem1031/pandas-pipelines-custom-transformers/blob/master/code/custom_transformers.py\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "class drop_missing_rows_columns(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, column_threshold=0.1, row_threshold=0.2):\n",
    "        \n",
    "        # attributes\n",
    "        self.column_threshold = column_threshold\n",
    "        self.row_threshold = row_threshold\n",
    "        self.cols_to_drop = []\n",
    "        self.rows_to_drop = []\n",
    "#         self.numeric_features     = X_train.select_dtypes(include=[np.number]).columns\n",
    "#         self.categorical_features = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        print('enter fitting drop_missing_rows_columns')\n",
    "        # identify missing columns and rows\n",
    "        self.cols_to_drop = X.columns[(X.isnull().sum()/X.shape[0]) > self.column_threshold].to_list()\n",
    "        self.rows_to_drop = X[(X.isnull().sum(axis=1)/X.shape[1]) > self.row_threshold].index.to_list()\n",
    "        print('finish fitting drop_missing_rows_columns')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        print('enter transforming drop_missing_rows_columns')\n",
    "        # missing columns threshold 0.1\n",
    "        X = X.drop(columns=self.cols_to_drop)\n",
    "        \n",
    "        # missing rows, threshold 0.2\n",
    "        X = X.drop(self.rows_to_drop)\n",
    "        \n",
    "        print('finish transforming drop_missing_rows_columns')\n",
    "        \n",
    "        return X\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missing = drop_missing_rows_columns() #column_threshold=0.1, row_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter fitting drop_missing_rows_columns\n",
      "finish fitting drop_missing_rows_columns\n",
      "enter transforming drop_missing_rows_columns\n",
      "finish transforming drop_missing_rows_columns\n"
     ]
    }
   ],
   "source": [
    "X_train = drop_missing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of features with specific data types for processing\n",
    "\n",
    "numeric_features     = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputer. Replace NaN with 'missing_value' for categorical fields.\n",
    "categorical_transformer  = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant')),\n",
    "                                           ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Standardise numerical fields\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble categorical and numerical pipelines\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
